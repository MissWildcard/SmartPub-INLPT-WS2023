{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch download Pubmed abstracts using the NCBI E-utilities and Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I will show you how NCBI E-utilities can be used to search for and download Pubmed abstracts. I use Python in conjunction with the NCBI E-utilities to download all the abstracts corresponding to a given search term and simultaneously parse the information contained in each abstract into a data science-friendly format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on what you search for in PubMed, you could be presented with thousands of abstracts that contain the keyword you used for your query (for example, try searching \"cancer\"). Finding the information you’re looking for can get a bit tedious when you have to manually click through each page of search results.\n",
    "\n",
    "Using the NCBI E-utilities (Entrez Programming Utilities, https://www.ncbi.nlm.nih.gov/books/NBK25499/), you can retrieve and download abstracts associated with a PubMed search without having to sift through the user interface. Even better, this tool doesn’t require any software–its completely URL based. You craft \"search\" and \"fetch\" commands as URLs and open them in your browser window to access the abstracts.\n",
    "\n",
    "We can automate the download process by programming a script in Python to construct the URLs, execute the \"search\" and \"fetch\" commands, and parse each part of the abstract (Authors, Journal, Date of publication, etc.) into a data file for downstream analysis. Text from each abstract can be analyzed to quickly extract numerical data or quantitative results.\n",
    "\n",
    "Below, I will give a brief tutorial about how the tools work, and the code needed to automate the process using Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the NCBI E-utilities work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two main E-util functions you will use are `esearch` and `efetch`.\n",
    "\n",
    "First, `esearch` runs a keyword search command on the PubMed database and retrieves IDs for each of the abstracts corresponding to the search. The actual information associated with the abstracts does not show up, only the IDs. You’re also given a `query key` and `web environment ID`. \n",
    "\n",
    "Then, you input the `query key` and `web environment ID` into an `efetch` call, which will “fetch” all the abstracts for that specific `esearch` query.\n",
    "\n",
    "Let’s say I want to search pubmed for \"Intelligence\".\n",
    "\n",
    "### Step 1. Craft your esearch URL\n",
    "\n",
    "Here is the URL required to execute a PubMed esearch for P2RY8:\n",
    "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=P2RY8&retmax=50&usehistory=y\n",
    "\n",
    "This was crafted by putting the following parameters together:\n",
    "* `http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?` is the backbone of the esearch function.\n",
    "* `db=pubmed` specifies that we will be searching the pubmed database. \n",
    "* `term=P2RY8` specifies what we will be searching pubmed for. Change this field to whatever you want to search for.\n",
    "* `retmax=50` specifies how many abstracts I want to return using the search.\n",
    "* `usehistory=y` will provide you with a QueryKey and WebEnv id that will let you fetch abstracts from this search.\n",
    "* The “&” signs are just used to separate the different conditions. Make sure to include it starting from after the `db=pubmed` argument.\n",
    "\n",
    "Copying and pasting the full URL into my web browser results in a webpage that looks like this (XML output):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![esearch_web_result.png](images/esearch_web_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Craft the efetch URL\n",
    "\n",
    "The next step is to execute an efetch command by constructing a new URL. Using the `WebEnv` and `QueryKey` information given in the above esearch result, I will type the following efetch command into my browser:\n",
    "\n",
    "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=NCID_1_5757184_130.14.18.48_9001_1579844644_2015135327_0MetA0_S_MegaStore&retstart=0&retmax=50&retmode=text&rettype=abstract\n",
    "\n",
    "Note: If you’re trying this right now, your esearch will have given you a different webenv variable. Make sure to input YOUR webenv variable in the efetch URL for it to work!\n",
    "\n",
    "Here is an explanation for each aspect of the link I constructed above:\n",
    "* `http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?` is the backbone for a efetch command. Notice that the only difference from this and an esearch command is the part after “/eutils/”.\n",
    "* `db=pubmed` specifies the database, again.\n",
    "* `query_key=1` specifies the number that was given in the “querykey” field in the esearch result.\n",
    "* `webenv=NCID_1_5757184_130.14.18.48_9001_1579844644_2015135327_0MetA0_S_MegaStore` specifies the ID that was given in the esearch result. \n",
    "* `retmode=text` specifies that I want the abstracts to be written out in print. \n",
    "* `rettype=abstract` specifies that I want abstracts shown, as opposed to other types of info that can be given from a PubMed search.\n",
    "\n",
    "After inputting this link, you should observe the following output as a plaintext webpage:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![efetch_web_result.png](images/efetch_web_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I can simply ctrl-F to sift through over three hundred abstracts. You can apply this simple two-step process whenever you’re tasked with searching through absurd amounts of Pubmed results.\n",
    "\n",
    "Below, I show you how to perform this process in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the E-utilities in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packages we need include `csv` to write to a csv file, `re` in order to use regular expressions to extract information from `esearch` results, `urllib` to open and read urls, and `time` in order to sleep for a couple seconds between requests so we don't get blocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import urllib\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to specify our parameters for the `esearch` and `efetch` calls. We can store each of the settings in their own individual variables in order to make it easy to customize the calls in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'intelligence'\n",
    "\n",
    "# common settings between esearch and efetch\n",
    "base_url = 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/'\n",
    "db = 'db=pubmed'\n",
    "\n",
    "# esearch specific settings\n",
    "search_eutil = 'esearch.fcgi?'\n",
    "search_term = '&term=' + query\n",
    "search_usehistory = '&usehistory=y'\n",
    "search_rettype = '&rettype=json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct the search url by simply combining together each of the variables into a long string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=intelligence&usehistory=y&rettype=json\n"
     ]
    }
   ],
   "source": [
    "search_url = base_url+search_eutil+db+search_term+search_usehistory+search_rettype\n",
    "print(search_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the full `esearch` url constructed, we can open the url using `urllib.request.urlopen()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = urllib.request.urlopen(search_url)\n",
    "search_data = f.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\\n<!DOCTYPE eSearchResult PUBLIC \"-//NLM//DTD esearch 20060628//EN\" \"https://eutils.ncbi.nlm.nih.gov/eutils/dtd/20060628/esearch.dtd\">\\n<eSearchResult><Count>323027</Count><RetMax>20</RetMax><RetStart>0</RetStart><QueryKey>1</QueryKey><WebEnv>MCID_657c84d9fe81845c6315a812</WebEnv><IdList>\\n<Id>38099452</Id>\\n<Id>38099400</Id>\\n<Id>38099383</Id>\\n<Id>38099340</Id>\\n<Id>38099205</Id>\\n<Id>38099199</Id>\\n<Id>38099176</Id>\\n<Id>38099140</Id>\\n<Id>38099136</Id>\\n<Id>38099089</Id>\\n<Id>38099088</Id>\\n<Id>38099081</Id>\\n<Id>38099049</Id>\\n<Id>38099026</Id>\\n<Id>38099022</Id>\\n<Id>38098952</Id>\\n<Id>38098921</Id>\\n<Id>38098857</Id>\\n<Id>38098822</Id>\\n<Id>38098788</Id>\\n</IdList><TranslationSet><Translation>     <From>intelligence</From>     <To>\"intelligence\"[MeSH Terms] OR \"intelligence\"[All Fields] OR \"intelligences\"[All Fields] OR \"intelligent\"[All Fields] OR \"intelligently\"[All Fields] OR \"intelligibilities\"[All Fields] OR \"intelligibility\"[All Fields] OR \"intelligible\"[All Fields]</To>    </Translation></TranslationSet><QueryTranslation>\"intelligence\"[MeSH Terms] OR \"intelligence\"[All Fields] OR \"intelligences\"[All Fields] OR \"intelligent\"[All Fields] OR \"intelligently\"[All Fields] OR \"intelligibilities\"[All Fields] OR \"intelligibility\"[All Fields] OR \"intelligible\"[All Fields]</QueryTranslation></eSearchResult>\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you can compare the raw text output of the esearch result with the image of what it looks like in the browser. You can see that the same syntax is used for the `WebEnv` and `QueryKey` sections. We will also need the total number of abstracts corresponding to the query, if we want to be able to retrieve all the abstracts. We can extract these items from the output using the regexes below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain total abstract count\n",
    "total_abstract_count = int(re.findall(\"<Count>(\\d+?)</Count>\",search_data)[0])\n",
    "\n",
    "# obtain webenv and querykey settings for efetch command\n",
    "fetch_webenv = \"&WebEnv=\" + re.findall (\"<WebEnv>(\\S+)<\\/WebEnv>\", search_data)[0]\n",
    "fetch_querykey = \"&query_key=\" + re.findall(\"<QueryKey>(\\d+?)</QueryKey>\",search_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323027"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_abstract_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are 32027 total abstracts that match the keyword \"Intelligence\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'&WebEnv=MCID_657c84d9fe81845c6315a812'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_webenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'&query_key=1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_querykey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the `WebEnv` and `QueryKey`, we can run an `efetch` command to obtain the abstracts. In order to do so, we must assign values to each of the parameters in a similar manner as we did above for esearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other efetch settings\n",
    "fetch_eutil = 'efetch.fcgi?'\n",
    "retmax = 20\n",
    "retstart = 0\n",
    "fetch_retstart = \"&retstart=\" + str(retstart)\n",
    "fetch_retmax = \"&retmax=\" + str(retmax)\n",
    "fetch_retmode = \"&retmode=text\"\n",
    "fetch_rettype = \"&rettype=abstract\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fully constructed efetch command using the above parameters, which should fetch 20 of the 56 total abstracts, is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c84d9fe81845c6315a812&retstart=0&retmax=20&retmode=text&rettype=abstract\n"
     ]
    }
   ],
   "source": [
    "fetch_url = base_url+fetch_eutil+db+fetch_querykey+fetch_webenv+fetch_retstart+fetch_retmax+fetch_retmode+fetch_rettype\n",
    "print(fetch_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can open the url the same way we did for esearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = urllib.request.urlopen (fetch_url)\n",
    "fetch_data = f.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. Chem Soc Rev. 2023 Dec 15. doi: 10.1039/d3cs00714f. Online ahead of print.\\n\\nElectrocatalysis of nitrogen pollution: transforming nitrogen waste into \\nhigh-value chemicals.\\n\\nWu Q(1), Zhu F(2), Wallace G(1), Yao X(2), Chen J(1).\\n\\nAuthor information:\\n(1)Intelligent Polymer Research Institute, Australian Institute for Innovative \\nMaterials, Innovation Campus, University of Wollongong, Squires Way, North \\nWollongong, NSW 2500, Australia. junc@uow.edu.au.\\n(2)School of Advanced Energy, Shenzhen Campus, Sun Yat-Sen University, Shenzhen, \\nGuangdong 518107, P. R. China. yaoxd3@mail.sysu.edu.cn.\\n\\nOn 16 June 2023, the United Nations Environment Programme highlighted the \\nseverity of nitrogen pollution faced by humans and called for joint action for \\nsustainable nitrogen use. Excess nitrogenous waste (NW: NO, NO2, NO2-, NO3-, \\netc.) mainly arises from the use of synthetic fertilisers, wastewater discharge, \\nand fossil fuel combustion. Although the amount of NW produced can be minimised \\nby reducing the use of nitrogen fertilisers and fossil fuels, the necessity to \\nfeed seven billion people on Earth limits the utility of this approach. Compared \\nto current industrial processes, electrocatalytic NW reduction or CO2-NW \\nco-reduction offers a potentially greener alternative for recycling NW and \\nproducing high-value chemicals. However, upgrading this technology to connect \\nupstream and downstream industrial chains is challenging. This viewpoint focuses \\non electrocatalytic NW reduction, a cutting-edge technology, and highlights the \\nchallenges in its practical application. It also discusses future directions to \\nmeet the requirements of upstream and downstream industries by optimising \\nproduction processes, including the pretreatment and supply of nitrogenous raw \\nmaterials (e.g. flue gas and sewage), design and macroscopic preparation of \\nelectrocatalysts, and upscaling of reactors and other auxiliary equipment.\\n\\nDOI: 10.1039/d3cs00714f\\nPMID: 38099452\\n\\n\\n2. Nanoscale. 2023 Dec 15. doi: 10.1039/d3nr04060g. Online ahead of print.\\n\\nA new triboelectric nanogenerator based on a multi-material stacking structure \\nachieves efficient power conversion from discrete mechanical movement.\\n\\nLuo J(1), Su Y(1)(2), Liu A(1), Dai G(1), Zhang X(1), Su X(1), Shao Y(1), Li \\nZ(1), Zhao X(1)(3), Zhao K(1)(4).\\n\\nAuthor information:\\n(1)School of Marine Engineering Equipment, Zhejiang Ocean University, Zhoushan \\n316022, China. syx@zjou.edu.cn.\\n(2)School of Electrical Engineering, Southwest Jiaotong University, Chengdu \\n611756, China.\\n(3)Ocean College, Zhejiang University, Zhoushan 316021, China.\\n(4)Laboratory of Polymers and Composites, Ningbo Institute of Materials \\nTechnology and Engineering, Chinese Academy of Sciences, Ningbo 315201, China.\\n\\nDue to its invaluable potential in discrete mechanical energy collection, TENG \\n(triboelectric nanogenerator) is considered to satisfy the power requirements of \\nintelligent electronic devices and drive the development of the Internet of \\nThin'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_data[1:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the text output, we see that individual abstracts are separated by 3 new lines (```\\n\\n\\n```). We can therefore use split() to generate a list in which each item is a separate abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splits the data into individual abstracts\n",
    "abstracts = fetch_data.split(\"\\n\\n\\n\")\n",
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we had set `retmax = 20`, we obtained 20 abstracts. Let's take a closer look at an individual abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Chem Soc Rev. 2023 Dec 15. doi: 10.1039/d3cs00714f. Online ahead of print.\\n\\nElectrocatalysis of nitrogen pollution: transforming nitrogen waste into \\nhigh-value chemicals.\\n\\nWu Q(1), Zhu F(2), Wallace G(1), Yao X(2), Chen J(1).\\n\\nAuthor information:\\n(1)Intelligent Polymer Research Institute, Australian Institute for Innovative \\nMaterials, Innovation Campus, University of Wollongong, Squires Way, North \\nWollongong, NSW 2500, Australia. junc@uow.edu.au.\\n(2)School of Advanced Energy, Shenzhen Campus, Sun Yat-Sen University, Shenzhen, \\nGuangdong 518107, P. R. China. yaoxd3@mail.sysu.edu.cn.\\n\\nOn 16 June 2023, the United Nations Environment Programme highlighted the \\nseverity of nitrogen pollution faced by humans and called for joint action for \\nsustainable nitrogen use. Excess nitrogenous waste (NW: NO, NO2, NO2-, NO3-, \\netc.) mainly arises from the use of synthetic fertilisers, wastewater discharge, \\nand fossil fuel combustion. Although the amount of NW produced can be minimised \\nby reducing the use of nitrogen fertilisers and fossil fuels, the necessity to \\nfeed seven billion people on Earth limits the utility of this approach. Compared \\nto current industrial processes, electrocatalytic NW reduction or CO2-NW \\nco-reduction offers a potentially greener alternative for recycling NW and \\nproducing high-value chemicals. However, upgrading this technology to connect \\nupstream and downstream industrial chains is challenging. This viewpoint focuses \\non electrocatalytic NW reduction, a cutting-edge technology, and highlights the \\nchallenges in its practical application. It also discusses future directions to \\nmeet the requirements of upstream and downstream industries by optimising \\nproduction processes, including the pretreatment and supply of nitrogenous raw \\nmaterials (e.g. flue gas and sewage), design and macroscopic preparation of \\nelectrocatalysts, and upscaling of reactors and other auxiliary equipment.\\n\\nDOI: 10.1039/d3cs00714f\\nPMID: 38099452'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the first abstract\n",
    "abstracts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the sections of the abstract are separated by 2 new lines in a row, denoted by `\\n\\n`. We can again use `split()` to further categorize each section of the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2. Nanoscale. 2023 Dec 15. doi: 10.1039/d3nr04060g. Online ahead of print.',\n",
       " 'A new triboelectric nanogenerator based on a multi-material stacking structure \\nachieves efficient power conversion from discrete mechanical movement.',\n",
       " 'Luo J(1), Su Y(1)(2), Liu A(1), Dai G(1), Zhang X(1), Su X(1), Shao Y(1), Li \\nZ(1), Zhao X(1)(3), Zhao K(1)(4).',\n",
       " 'Author information:\\n(1)School of Marine Engineering Equipment, Zhejiang Ocean University, Zhoushan \\n316022, China. syx@zjou.edu.cn.\\n(2)School of Electrical Engineering, Southwest Jiaotong University, Chengdu \\n611756, China.\\n(3)Ocean College, Zhejiang University, Zhoushan 316021, China.\\n(4)Laboratory of Polymers and Composites, Ningbo Institute of Materials \\nTechnology and Engineering, Chinese Academy of Sciences, Ningbo 315201, China.',\n",
       " 'Due to its invaluable potential in discrete mechanical energy collection, TENG \\n(triboelectric nanogenerator) is considered to satisfy the power requirements of \\nintelligent electronic devices and drive the development of the Internet of \\nThings (IoT). Nowadays, the promotion of TENGs has been hindered due to the \\nlimitation of their output performance and service life. Herein, a brand new \\ntriboelectric nanogenerator based on a multi-material stacking structure is \\nproposed. By stacking various triboelectric materials in a specific order, a \\nspecial charge balance state could be achieved inside the system such that the \\nconductive layer generates more induced charges, and the output performance is \\nsignificantly enhanced. Besides, due to the usage of the electropositive \\nelastomer PU (polyurethane sponge), the design also effectively alleviates \\nabrasion on the contact surface and adjusts its own output according to \\ndifferent compression environments. The experimental results show that the \\nstacked PTFE/FKM/PU TENG (PFP-TENG) presents a more than 50% increase in \\ntransferred charge and almost 5 times the current output compared with the \\ngeneral contact-separation type TENG. When connected to the application circuit, \\nthe maximum output power reached 10.2 W m-2 and 145.2 W m-3, and more than 1400 \\nLEDs could be easily lit. Finally, the PFP-TENG was also used to collect \\nmechanical energy from simple motion and realize considerable power generation. \\nThis study not only provides new ideas for the design of TENGs by reasoning the \\ntheoretical model but also presents improved output performance, thus \\nexemplifying the strong potential of this design in developing a \\npower-generation device that can collect discrete mechanical energy.',\n",
       " 'DOI: 10.1039/d3nr04060g\\nPMID: 38099400']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_abstract = abstracts[1].split(\"\\n\\n\")\n",
    "split_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We see that the abstract has been split into 7 different items of information, corresponding to the journal name, date, authors, etc. Knowing this, we can construct a data frame in which each row represents an abstract, and each column represents a section of the abstract (journal, date, authors, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a loop to fetch all abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were 56 total abstracts corresponding to the keyword search P2RY8. Above, we have only processed the first 20. In order to obtain all of the abstracts, we can construct a loop that will call ```efetch``` while incrementing ```retstart``` by 20 each iteration, until all the abstracts have been downloaded and added to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the esearch command:\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=intelligence&usehistory=y&rettype=json\n",
      "\n",
      "this is efetch run number 1\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=0&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 492 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 2\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=500&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 984 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 3\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=1000&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 1476 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 4\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=1500&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 1968 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 5\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=2000&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 2460 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 6\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=2500&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 2952 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 7\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=3000&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 3444 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 8\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=3500&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 3936 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 9\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=4000&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 4428 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 10\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=4500&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 4920 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 11\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=5000&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 5412 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 12\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=5500&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 5904 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 13\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=6000&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 6396 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 14\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=6500&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 6888 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 15\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=7000&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 7380 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 16\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=7500&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 7872 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 17\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=8000&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 8364 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 18\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=8500&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 8856 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 19\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=9000&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 9348 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 20\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=9500&retmax=500&retmode=text&rettype=abstract\n",
      "a total of 9839 abstracts have been downloaded.\n",
      "\n",
      "this is efetch run number 21\n",
      "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=MCID_657c8717129b4b17846fd004&retstart=10000&retmax=500&retmode=text&rettype=abstract\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 400: Bad Request",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 43>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(fetch_url)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# open the efetch url\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfetch_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m fetch_data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# split the data into individual abstracts\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:555\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    553\u001b[0m     http_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    554\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, proto, meth_name) \u001b[38;5;241m+\u001b[39m args\n\u001b[1;32m--> 555\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:747\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[1;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[0;32m    744\u001b[0m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    745\u001b[0m fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    560\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 400: Bad Request"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import urllib\n",
    "from time import sleep\n",
    "\n",
    "query = \"intelligence\"\n",
    "\n",
    "# common settings between esearch and efetch\n",
    "base_url = 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/'\n",
    "db = 'db=pubmed'\n",
    "\n",
    "# esearch settings\n",
    "search_eutil = 'esearch.fcgi?'\n",
    "search_term = '&term=' + query\n",
    "search_usehistory = '&usehistory=y'\n",
    "search_rettype = '&rettype=json'\n",
    "\n",
    "# call the esearch command for the query and read the web result\n",
    "search_url = base_url+search_eutil+db+search_term+search_usehistory+search_rettype\n",
    "print(\"this is the esearch command:\\n\" + search_url + \"\\n\")\n",
    "f = urllib.request.urlopen (search_url)\n",
    "search_data = f.read().decode('utf-8')\n",
    "\n",
    "# extract the total abstract count\n",
    "total_abstract_count = int(re.findall(\"<Count>(\\d+?)</Count>\",search_data)[0])\n",
    "\n",
    "# efetch settings\n",
    "fetch_eutil = 'efetch.fcgi?'\n",
    "retmax = 500\n",
    "retstart = 0\n",
    "fetch_retmode = \"&retmode=text\"\n",
    "fetch_rettype = \"&rettype=abstract\"\n",
    "\n",
    "# obtain webenv and querykey settings from the esearch results\n",
    "fetch_webenv = \"&WebEnv=\" + re.findall (\"<WebEnv>(\\S+)<\\/WebEnv>\", search_data)[0]\n",
    "fetch_querykey = \"&query_key=\" + re.findall(\"<QueryKey>(\\d+?)</QueryKey>\",search_data)[0]\n",
    "\n",
    "# call efetch commands using a loop until all abstracts are obtained\n",
    "run = True\n",
    "all_abstracts = list()\n",
    "loop_counter = 1\n",
    "\n",
    "while run:\n",
    "    print(\"this is efetch run number \" + str(loop_counter))\n",
    "    loop_counter += 1\n",
    "    fetch_retstart = \"&retstart=\" + str(retstart)\n",
    "    fetch_retmax = \"&retmax=\" + str(retmax)\n",
    "    # create the efetch url\n",
    "    fetch_url = base_url+fetch_eutil+db+fetch_querykey+fetch_webenv+fetch_retstart+fetch_retmax+fetch_retmode+fetch_rettype\n",
    "    print(fetch_url)\n",
    "    # open the efetch url\n",
    "    f = urllib.request.urlopen (fetch_url)\n",
    "    fetch_data = f.read().decode('utf-8')\n",
    "    # split the data into individual abstracts\n",
    "    abstracts = fetch_data.split(\"\\n\\n\\n\")\n",
    "    # append to the list all_abstracts\n",
    "    all_abstracts = all_abstracts+abstracts\n",
    "    print(\"a total of \" + str(len(all_abstracts)) + \" abstracts have been downloaded.\\n\")\n",
    "    # wait 2 seconds so we don't get blocked\n",
    "    sleep(2)\n",
    "    # update retstart to download the next chunk of abstracts\n",
    "    retstart = retstart + retmax\n",
    "    if retstart > total_abstract_count:\n",
    "        run = False\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script above should display the esearch command, as well as each individual efetch command used to download the data. We observe that 3 efetch commands were called, and that a total of 56 abstracts were downloaded and stored in the list `all_abstracts`.\n",
    "\n",
    "You might be wondering, why did we need a loop in the first place? Couldn't we have just set `retmax` to 56 and just run `efetch` only once? For this particular example, yes. However, the maximum value for `retmax` is 500. For other keyword searches with over 500 abstracts, we must set retmax to 500 and loop until all the abstracts are downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9839"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will split each abstract from `all_abstracts` into the categories: 'Journal', 'Title', 'Authors', 'Author_Information', 'Abstract', 'DOI', and 'Misc'. After splitting each abstract, we will write the information to a csv file for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"abstracts.csv\", \"wt\",encoding='utf-8', newline='') as abstracts_file:\n",
    "    abstract_writer = csv.writer(abstracts_file)\n",
    "    abstract_writer.writerow(['Journal', 'Title', 'Authors', 'Author_Information', 'Abstract', 'DOI', 'Misc'])\n",
    "    #For each abstract, split into categories and write it to the csv file\n",
    "    for abstract in all_abstracts:\n",
    "        #To obtain categories, split every double newline.\n",
    "        split_abstract = abstract.split(\"\\n\\n\")\n",
    "        abstract_writer.writerow(split_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the resulting csv file, we notice that some abstracts are missing pieces of information. Some lack the author information, and some lack the abstract text entirely. We are only interested in abstracts with complete information, we should segregate the incomplete abstracts into a separate file. To do this, we can just include a simple if/else clause and write to two files. The code below should do so, in which the incomplete abstracts will be written to a file called `partial_abstracts.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"abstracts.csv\", \"wt\",encoding='utf-8', newline='') as abstracts_file, open (\"partial_abstracts.csv\", \"wt\",encoding='utf-8', newline='') as partial_abstracts:\n",
    "    # csv writer for full abstracts\n",
    "    abstract_writer = csv.writer(abstracts_file)\n",
    "    abstract_writer.writerow(['Journal', 'Title', 'Authors', 'Author_Information', 'Abstract', 'DOI', 'Misc'])\n",
    "    # csv writer for partial abstracts\n",
    "    partial_abstract_writer = csv.writer(partial_abstracts)\n",
    "    #For each abstract, split into categories and write it to the csv file\n",
    "    for abstract in all_abstracts:\n",
    "        #To obtain categories, split every double newline.\n",
    "        split_abstract = abstract.split(\"\\n\\n\")\n",
    "        if len(split_abstract) > 5:\n",
    "            abstract_writer.writerow(split_abstract)\n",
    "        else:\n",
    "            partial_abstract_writer.writerow(split_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I have shown you how to use the NCBI E-utilities `esearch` and `efetch` to download abstracts from PubMed, as well as how to write a Python script to batch download all abstracts corresponding to a keyword search.\n",
    "\n",
    "The file `pubmed_extractor.py` in this repository contains the Python script that we wrote above, but will also allow the user to input their desired keyword search. Running the script by typing `python pubmed_extractor.py` into your terminal should prompt you for a keyword to download PubMed abstracts for. The script will then dump your abstracts into `abstracts.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "Sayers E. The E-utilities In-Depth: Parameters, Syntax and More. 2009 May 29. In: Entrez Programming Utilities Help. Bethesda (MD): National Center for Biotechnology Information (US); 2010-.Available from: http://www.ncbi.nlm.nih.gov/books/NBK25499/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
